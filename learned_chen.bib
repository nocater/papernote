% Encoding: UTF-8

@InCollection{Krizhevsky2012,
  author    = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25},
  year      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  pages     = {1097--1105},
  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  comment   = {经典论文AlexNet Google 2012年ImageNet排名第一},
  file      = {:paper/imagenetclassificationCNN.pdf:PDF},
  keywords  = {rank4},
  owner     = {[C]NIPS},
}

@Article{Srivastava2014,
  author    = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title     = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal   = {Journal of Machine Learning Research},
  year      = {2014},
  volume    = {15},
  pages     = {1929-1958},
  url       = {http://jmlr.org/papers/v15/srivastava14a.html},
  comment   = {Dropout技术},
  file      = {:paper/JMLRdropout.pdf:PDF},
  keywords  = {rank5},
  owner     = {[A]JMLR},
  timestamp = {2018-04-18},
}

@Article{,
  author    = {DevanshArpit},
  title     = {A Closer Look at Memorization in Deep Networks},
  journal   = {NULL},
  year      = {2017},
  date      = {2017-01-01},
  url       = {https://arxiv.org/pdf/1706.05394},
  abstract  = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns ﬁrst. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasetswithoutcompromisinggeneralizationon real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performanceofdeepnetworkswhentrainedwith gradient based methods because training data itself plays an important role in determining the degree of memorization.
},
  comment   = {DNN先学习后记忆},
  file      = {:paper/ACloserLookatMemorizationinDeepNetworks.pdf:PDF},
  keywords  = {rank3},
  owner     = {NULL},
  timestamp = {2018-03-26},
}

@InProceedings{Ioffe2015,
  author    = {Ioffe, Sergey and Szegedy, Christian},
  title     = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  booktitle = {NULL},
  year      = {2015},
  language  = {English},
  volume    = {1},
  note      = {Classification models;Covariate shifts;Deep neural networks;Higher learning;Learning rates;Model architecture;Network training;State of the art;},
  pages     = {448-456},
  url       = {https://www.engineeringvillage.com/search/doc/abstract.url?&pageType=quickSearch&usageZone=resultslist&usageOrigin=searchresults&searchtype=Quick&SEARCHID=fe1865a8Ma2d0M4c74Ma439M0d1910fd4650&DOCINDEX=1&ignore_docid=cpx_626caa911554ab001deM7b6c10178163171&database=1&format=quickSearchAbstractFormat&tagscope=&displayPagination=yes},
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  address   = {Lile, France},
  comment   = {Batch Normalization},
  copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
  file      = {:paper/BatchNormalization.pdf:PDF},
  journal   = {32nd International Conference on Machine Learning, ICML 2015},
  key       = {Image classification},
  keywords  = {Artificial intelligence;Learning systems;, rank5},
  owner     = {[A]ICML},
  timestamp = {2018-05-13},
}

@Article{Efficien95:online,
  author       = {Rui Min},
  title        = {Efficient Detection of Occlusion prior to Robust Face Recognition},
  journal      = {null},
  year         = {2014},
  note         = {(Accessed on 07/07/2018)},
  url          = {https://www.hindawi.com/journals/tswj/2014/519158/abs/},
  comment      = {人脸检测{遮挡和人脸在上下不同部分，再提取未遮挡人脸进行特征选取分类}},
  file         = {:paper/EfficientDetectionofOcclusionPriorToRobustFaceRecongnition.pdf:PDF},
  howpublished = {\url{https://www.hindawi.com/journals/tswj/2014/519158/abs/}},
  keywords     = {rank2},
  timestamp    = {2018-07-07},
}

@InProceedings{20181805133229,
  author    = {Singh, Amarjot and Patii, Devendra and Reddy, G. Meghana and Omkar, S.N.},
  title     = {Disguised Face Identification (DFI) with Facial KeyPoints Using Spatial Fusion Convolutional Network},
  booktitle = {null},
  year      = {2018},
  language  = {English},
  volume    = {2018-January},
  note      = {Annotated datasets;Classification methods;Classification performance;Convolutional networks;Face identification;Keypoint detection;Learning architectures;Learning frameworks;},
  pages     = {1648 - 1655},
  url       = {http://dx.doi.org/10.1109/ICCVW.2017.193},
  abstract  = {Disguised face identification (DFI) is an extremely challenging problem due to the numerous variations that can be introduced using different disguises. This paper introduces a deep learning framework to first detect 14 facial key-points which are then utilized to perform disguised face identification. Since the training of deep learning architectures relies on large annotated datasets, two annotated facial key-points datasets are introduced. The effectiveness of the facial keypoint detection framework is presented for each keypoint. The superiority of the key-point detection framework is also demonstrated by a comparison with other deep networks. The effectiveness of classification performance is also demonstrated by comparison with the state-of-the-art face disguise classification methods.<br/> &copy; 2017 IEEE.},
  address   = {Venice, Italy},
  comment   = {使用CNN识别人脸14个关键点},
  copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
  file      = {:paper/Singh_Disguised_Face_Identification_ICCV_2017_paper.pdf:PDF},
  journal   = {Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
  key       = {Face recognition},
  keywords  = {Computer vision;Damage detection;Deep learning;, rank3},
  owner     = {[C]ICCVW 2017},
}

@Article{ZHANG20171,
  author    = {Hengmin Zhang and Jian Yang and Jianchun Xie and Jianjun Qian and Bob Zhang},
  title     = {Weighted sparse coding regularized nonconvex matrix regression for robust face recognition},
  journal   = {Information Sciences},
  year      = {2017},
  volume    = {394-395},
  pages     = {1 - 17},
  issn      = {0020-0255},
  doi       = {https://doi.org/10.1016/j.ins.2017.02.020},
  url       = {http://www.sciencedirect.com/science/article/pii/S0020025517304954},
  comment   = {[牛] -WSC能将位置结构和相似关系结合成稀疏表示},
  keywords  = {Nonconvex matrix regression, Weighted sparse coding, Inexact augmented lagrange multiplier method, Face recognition, rank5},
  owner     = {[A]Information Sciences},
  timestamp = {2018-07-07},
}

@Article{726791,
  author    = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
  title     = {Gradient-based learning applied to document recognition},
  journal   = {Proceedings of the IEEE},
  year      = {1998},
  volume    = {86},
  number    = {11},
  month     = {Nov},
  pages     = {2278-2324},
  issn      = {0018-9219},
  doi       = {10.1109/5.726791},
  comment   = {经典论文LeNet},
  file      = {:paper/LeNet.pdf:PDF},
  keywords  = {backpropagation;convolution;multilayer perceptrons;optical character recognition;2D shape variability;GTN;back-propagation;cheque reading;complex decision surface synthesis;convolutional neural network character recognizers;document recognition;document recognition systems;field extraction;gradient based learning technique;gradient-based learning;graph transformer networks;handwritten character recognition;handwritten digit recognition task;high-dimensional patterns;language modeling;multilayer neural networks;multimodule systems;performance measure minimization;segmentation recognition;Character recognition;Feature extraction;Hidden Markov models;Machine learning;Multi-layer neural network;Neural networks;Optical character recognition software;Optical computing;Pattern recognition;Principal component analysis, rank5},
  owner     = {[A]IEEE},
  timestamp = {2018-07-11},
}

@Article{DBLP:journals/corr/SimonyanZ14a,
  author        = {Karen Simonyan and Andrew Zisserman},
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1409.1556},
  eprint        = {1409.1556},
  url           = {http://arxiv.org/abs/1409.1556},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
  comment       = {经典论文VGG},
  file          = {:paper/VGG.pdf:PDF},
  keywords      = {rank4},
  timestamp     = {Wed, 07 Jun 2017 01:00:00 +0200},
}

@Article{DBLP:journals/corr/ZhangYS17aa,
  author        = {Shuai Zhang and Lina Yao and Aixin Sun},
  title         = {Deep Learning based Recommender System: {A} Survey and New Perspectives},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1707.07435},
  eprint        = {1707.07435},
  url           = {http://arxiv.org/abs/1707.07435},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/ZhangYS17aa},
  comment       = {基于DL的推荐系统综述},
  file          = {基于深度学习的推荐系统综述:paper/DeepLearningbasedRecommenderSystemASurveyandNewPerspective.pdf:PDF},
  owner         = {[A]CoRR},
  timestamp     = {Tue, 08 Aug 2017 13:30:53 +0200},
}

@InProceedings{20160902042580,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title     = {Going deeper with convolutions},
  booktitle = {cvpr},
  year      = {2015},
  language  = {English},
  volume    = {07-12-June-2015},
  note      = {Architectural decision;Computational budget;Computing resource;Deep convolutional neural networks;Deep networks;Multi-scale;State of the art;Visual recognition;},
  pages     = {1-9},
  url       = {http://dx.doi.org/10.1109/CVPR.2015.7298594},
  abstract  = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.<br/> &copy; 2015 IEEE.},
  address   = {Boston, MA, United states},
  comment   = {经典论文Inception},
  copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
  file      = {:paper/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf:PDF},
  issn      = {10636919},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  key       = {Network architecture},
  keywords  = {Budget control;Computer vision;Convolution;Deep neural networks;Network layers;Neural networks;},
  owner     = {[C]CVPR},
  timestamp = {2018-08-03},
}

@InProceedings{Szegedy2015,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title     = {Going deeper with convolutions},
  booktitle = {cvpr},
  year      = {2015},
  language  = {English},
  volume    = {07-12-June-2015},
  note      = {Architectural decision;Computational budget;Computing resource;Deep convolutional neural networks;Deep networks;Multi-scale;State of the art;Visual recognition;},
  pages     = {1 - 9},
  url       = {http://dx.doi.org/10.1109/CVPR.2015.7298594},
  abstract  = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.<br/> &copy; 2015 IEEE.},
  address   = {Boston, MA, United states},
  comment   = {经典论文RestNet},
  copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
  file      = {:paper/ResNet_He_CVPR_2016.pdf:PDF},
  issn      = {10636919},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  key       = {Network architecture},
  keywords  = {Budget control;Computer vision;Convolution;Deep neural networks;Network layers;Neural networks;, rank4},
  owner     = {[C]CVPR},
  timestamp = {2018-08-03},
}

@InProceedings{Ding:2017:BFR:3132847.3132941,
  author    = {Ding, Daizong and Zhang, Mi and Li, Shao-Yuan and Tang, Jie and Chen, Xiaotie and Zhou, Zhi-Hua},
  title     = {BayDNN: Friend Recommendation with Bayesian Personalized Ranking Deep Neural Network},
  booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  year      = {2017},
  series    = {CIKM '17},
  publisher = {ACM},
  location  = {Singapore, Singapore},
  isbn      = {978-1-4503-4918-5},
  pages     = {1479--1488},
  doi       = {10.1145/3132847.3132941},
  url       = {http://doi.acm.org/10.1145/3132847.3132941},
  acmid     = {3132941},
  address   = {New York, NY, USA},
  comment   = {BayDNN网络结构进行好友推荐},
  file      = {:paper/BayDNN.pdf:PDF},
  keywords  = {bayesian personalized ranking deep neural network, pre-training strategy, probabilistic model, rank3},
  numpages  = {10},
  owner     = {[C]ACM},
}

@InBook{Bengio2006,
  author    = {Bengio, Yoshua and Schwenk, Holger and Sen{\'e}cal, Jean-S{\'e}bastien and Morin, Fr{\'e}deric and Gauvain, Jean-Luc},
  title     = {Neural Probabilistic Language Models},
  booktitle = {Innovations in Machine Learning: Theory and Applications},
  year      = {2006},
  editor    = {Holmes, Dawn E. and Jain, Lakhmi C.},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {978-3-540-33486-6},
  pages     = {137--186},
  doi       = {10.1007/3-540-33486-6_6},
  url       = {https://doi.org/10.1007/3-540-33486-6_6},
  abstract  = {A central goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on several methods to speed-up both training and probability computation, as well as comparative experiments to evaluate the improvements brought by these techniques. We finally describe the incorporation of this new language model into a state-of-the-art speech recognizer of conversational speech.},
  address   = {Berlin, Heidelberg},
  comment   = {经典NNLM模型},
  file      = {NLP-NNLM:paper/NLP/NNLM-ANeuralProbabilisticLanguageModel.pdf:PDF},
  keywords  = {rank4},
}

@Article{DBLP:journals/corr/abs-1301-3781,
  author        = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  title         = {Efficient Estimation of Word Representations in Vector Space},
  journal       = {CoRR},
  year          = {2013},
  volume        = {abs/1301.3781},
  eprint        = {1301.3781},
  url           = {http://arxiv.org/abs/1301.3781},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1301-3781},
  comment       = {经典word2vec},
  file          = {NLP-word2vec:paper/NLP/word2vec.pdf:PDF},
  keywords      = {rank4},
  timestamp     = {Mon, 13 Aug 2018 16:48:33 +0200},
}

@Comment{jabref-meta: databaseType:biblatex;}
