% Encoding: UTF-8

@InCollection{Krizhevsky2012,
  author    = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25},
  year      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  pages     = {1097--1105},
  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  comment   = {Google 2012年ImageNet排名第一},
  file      = {:paper/imagenetclassificationCNN.pdf:PDF},
  keywords  = {rank4},
  owner     = {[C]NIPS},
}

@Article{Srivastava2014,
  author    = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title     = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal   = {Journal of Machine Learning Research},
  year      = {2014},
  volume    = {15},
  pages     = {1929-1958},
  url       = {http://jmlr.org/papers/v15/srivastava14a.html},
  comment   = {Dropout技术},
  file      = {:paper/JMLRdropout.pdf:PDF},
  keywords  = {rank5},
  owner     = {[A]JMLR},
  timestamp = {2018-04-18},
}

@Article{,
  author    = {DevanshArpit},
  title     = {A Closer Look at Memorization in Deep Networks},
  journal   = {NULL},
  year      = {2017},
  date      = {2017-01-01},
  url       = {https://arxiv.org/pdf/1706.05394},
  abstract  = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns ﬁrst. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasetswithoutcompromisinggeneralizationon real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performanceofdeepnetworkswhentrainedwith gradient based methods because training data itself plays an important role in determining the degree of memorization.
},
  comment   = {DNN先学习后记忆},
  file      = {:paper/ACloserLookatMemorizationinDeepNetworks.pdf:PDF},
  keywords  = {rank3},
  owner     = {NULL},
  timestamp = {2018-03-26},
}

@InProceedings{Ioffe2015,
  author    = {Ioffe, Sergey and Szegedy, Christian},
  title     = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  booktitle = {NULL},
  year      = {2015},
  language  = {English},
  volume    = {1},
  note      = {Classification models;Covariate shifts;Deep neural networks;Higher learning;Learning rates;Model architecture;Network training;State of the art;},
  pages     = {448-456},
  url       = {https://www.engineeringvillage.com/search/doc/abstract.url?&pageType=quickSearch&usageZone=resultslist&usageOrigin=searchresults&searchtype=Quick&SEARCHID=fe1865a8Ma2d0M4c74Ma439M0d1910fd4650&DOCINDEX=1&ignore_docid=cpx_626caa911554ab001deM7b6c10178163171&database=1&format=quickSearchAbstractFormat&tagscope=&displayPagination=yes},
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  address   = {Lile, France},
  comment   = {Batch Normalization},
  copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
  file      = {:paper/BatchNormalization.pdf:PDF},
  journal   = {32nd International Conference on Machine Learning, ICML 2015},
  key       = {Image classification},
  keywords  = {Artificial intelligence;Learning systems;, rank5},
  owner     = {[A]ICML},
  timestamp = {2018-05-13},
}

@Article{Efficien95:online,
  author       = {Rui Min},
  title        = {Efficient Detection of Occlusion prior to Robust Face Recognition},
  journal      = {null},
  year         = {2014},
  note         = {(Accessed on 07/07/2018)},
  url          = {https://www.hindawi.com/journals/tswj/2014/519158/abs/},
  comment      = {人脸检测{遮挡和人脸在上下不同部分，再提取未遮挡人脸进行特征选取分类}},
  file         = {:paper/EfficientDetectionofOcclusionPriorToRobustFaceRecongnition.pdf:PDF},
  howpublished = {\url{https://www.hindawi.com/journals/tswj/2014/519158/abs/}},
  keywords     = {rank2},
  timestamp    = {2018-07-07},
}

@InProceedings{20181805133229,
  author    = {Singh, Amarjot and Patii, Devendra and Reddy, G. Meghana and Omkar, S.N.},
  title     = {Disguised Face Identification (DFI) with Facial KeyPoints Using Spatial Fusion Convolutional Network},
  booktitle = {null},
  year      = {2018},
  language  = {English},
  volume    = {2018-January},
  note      = {Annotated datasets;Classification methods;Classification performance;Convolutional networks;Face identification;Keypoint detection;Learning architectures;Learning frameworks;},
  pages     = {1648 - 1655},
  url       = {http://dx.doi.org/10.1109/ICCVW.2017.193},
  abstract  = {Disguised face identification (DFI) is an extremely challenging problem due to the numerous variations that can be introduced using different disguises. This paper introduces a deep learning framework to first detect 14 facial key-points which are then utilized to perform disguised face identification. Since the training of deep learning architectures relies on large annotated datasets, two annotated facial key-points datasets are introduced. The effectiveness of the facial keypoint detection framework is presented for each keypoint. The superiority of the key-point detection framework is also demonstrated by a comparison with other deep networks. The effectiveness of classification performance is also demonstrated by comparison with the state-of-the-art face disguise classification methods.<br/> &copy; 2017 IEEE.},
  address   = {Venice, Italy},
  comment   = {使用CNN识别人脸14个关键点},
  copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
  file      = {:paper/Singh_Disguised_Face_Identification_ICCV_2017_paper.pdf:PDF},
  journal   = {Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
  key       = {Face recognition},
  keywords  = {Computer vision;Damage detection;Deep learning;, rank3},
  owner     = {[C]ICCVW 2017},
}

@Article{ZHANG20171,
  author    = {Hengmin Zhang and Jian Yang and Jianchun Xie and Jianjun Qian and Bob Zhang},
  title     = {Weighted sparse coding regularized nonconvex matrix regression for robust face recognition},
  journal   = {Information Sciences},
  year      = {2017},
  volume    = {394-395},
  pages     = {1 - 17},
  issn      = {0020-0255},
  doi       = {https://doi.org/10.1016/j.ins.2017.02.020},
  url       = {http://www.sciencedirect.com/science/article/pii/S0020025517304954},
  comment   = {[牛] -WSC能将位置结构和相似关系结合成稀疏表示},
  keywords  = {Nonconvex matrix regression, Weighted sparse coding, Inexact augmented lagrange multiplier method, Face recognition, rank5},
  owner     = {[A]Information Sciences},
  timestamp = {2018-07-07},
}

@Article{726791,
  author    = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
  title     = {Gradient-based learning applied to document recognition},
  journal   = {Proceedings of the IEEE},
  year      = {1998},
  volume    = {86},
  number    = {11},
  month     = {Nov},
  pages     = {2278-2324},
  issn      = {0018-9219},
  doi       = {10.1109/5.726791},
  comment   = {经典论文LeNet},
  file      = {:paper/LeNet.pdf:PDF},
  keywords  = {backpropagation;convolution;multilayer perceptrons;optical character recognition;2D shape variability;GTN;back-propagation;cheque reading;complex decision surface synthesis;convolutional neural network character recognizers;document recognition;document recognition systems;field extraction;gradient based learning technique;gradient-based learning;graph transformer networks;handwritten character recognition;handwritten digit recognition task;high-dimensional patterns;language modeling;multilayer neural networks;multimodule systems;performance measure minimization;segmentation recognition;Character recognition;Feature extraction;Hidden Markov models;Machine learning;Multi-layer neural network;Neural networks;Optical character recognition software;Optical computing;Pattern recognition;Principal component analysis, rank4},
  owner     = {[A]IEEE},
  timestamp = {2018-07-11},
}

@Comment{jabref-meta: databaseType:biblatex;}
