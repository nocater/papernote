% Encoding: UTF-8

@InProceedings{DBLP:conf/emnlp/PenningtonSM14,
  author    = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  title     = {Glove: Global Vectors for Word Representation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2014, October 25-29, 2014, Doha, Qatar, {A} meeting of SIGDAT, a Special Interest Group of the {ACL}},
  year      = {2014},
  pages     = {1532--1543},
  url       = {http://aclweb.org/anthology/D/D14/D14-1162.pdf},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/emnlp/PenningtonSM14},
  comment   = {经典论文Glove},
  crossref  = {DBLP:conf/emnlp/2014},
  keywords  = {rank3},
  timestamp = {Sat, 15 Nov 2014 14:45:18 +0100},
}

@InProceedings{DBLP:conf/emnlp/SchnabelLMJ15,
  author    = {Tobias Schnabel and Igor Labutov and David M. Mimno and Thorsten Joachims},
  title     = {Evaluation methods for unsupervised word embeddings},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2015, Lisbon, Portugal, September 17-21, 2015},
  year      = {2015},
  pages     = {298--307},
  url       = {http://aclweb.org/anthology/D/D15/D15-1036.pdf},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/emnlp/SchnabelLMJ15},
  comment   = {非监督词向量的评估方式，作者想要统一评估},
  crossref  = {DBLP:conf/emnlp/2015},
  file      = {:paper/NLP/Evaluation methods for unsupervised word embeddings.pdf:PDF},
  timestamp = {Wed, 14 Oct 2015 09:56:24 +0200},
}

@InProceedings{DBLP:conf/emnlp/ChenM14,
  author    = {Danqi Chen and Christopher D. Manning},
  title     = {A Fast and Accurate Dependency Parser using Neural Networks},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2014, October 25-29, 2014, Doha, Qatar, {A} meeting of SIGDAT, a Special Interest Group of the {ACL}},
  year      = {2014},
  pages     = {740--750},
  url       = {http://aclweb.org/anthology/D/D14/D14-1082.pdf},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/emnlp/ChenM14},
  comment   = {[NLP]使用神经网络来做依赖解析，速度效果非常好。},
  crossref  = {DBLP:conf/emnlp/2014},
  keywords  = {rank4},
  timestamp = {Sat, 15 Nov 2014 14:45:18 +0100},
}

@InProceedings{DBLP:conf/emnlp/LuongPM15,
  author    = {Thang Luong and Hieu Pham and Christopher D. Manning},
  title     = {Effective Approaches to Attention-based Neural Machine Translation},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2015, Lisbon, Portugal, September 17-21, 2015},
  year      = {2015},
  pages     = {1412--1421},
  url       = {http://aclweb.org/anthology/D/D15/D15-1166.pdf},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/emnlp/LuongPM15},
  comment   = {Attention模型{global,local}},
  crossref  = {DBLP:conf/emnlp/2015},
  file      = {:paper/NLP/EffectiveApproachesToAttention-basedNeuralMachineTranslation.pdf:PDF},
  keywords  = {rank3},
  timestamp = {Wed, 14 Oct 2015 09:56:24 +0200},
}

@InProceedings{DBLP:conf/acl/SeeLM17,
  author    = {Abigail See and Peter J. Liu and Christopher D. Manning},
  title     = {Get To The Point: Summarization with Pointer-Generator Networks},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers},
  year      = {2017},
  pages     = {1073--1083},
  doi       = {10.18653/v1/P17-1099},
  url       = {https://doi.org/10.18653/v1/P17-1099},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/acl/SeeLM17},
  comment   = {Pointer模型，解决OOV问题},
  crossref  = {DBLP:conf/acl/2017-1},
  file      = {:paper/NLP/GetToThePoint.pdf:PDF},
  keywords  = {rank3},
  timestamp = {Fri, 04 Aug 2017 16:38:24 +0200},
}

@InCollection{Krizhevsky2012,
  author    = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25},
  year      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  pages     = {1097--1105},
  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  comment   = {经典论文AlexNet Google 2012年ImageNet排名第一},
  file      = {:paper/imagenetclassificationCNN.pdf:PDF},
  keywords  = {rank4},
  owner     = {[C]NIPS},
}

@Article{Srivastava2014,
  author    = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title     = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal   = {Journal of Machine Learning Research},
  year      = {2014},
  volume    = {15},
  pages     = {1929-1958},
  url       = {http://jmlr.org/papers/v15/srivastava14a.html},
  comment   = {Dropout技术},
  file      = {:paper/JMLRdropout.pdf:PDF},
  keywords  = {rank5},
  owner     = {[A]JMLR},
  timestamp = {2018-04-18},
}

@Article{,
  author    = {DevanshArpit},
  title     = {A Closer Look at Memorization in Deep Networks},
  journal   = {NULL},
  year      = {2017},
  date      = {2017-01-01},
  url       = {https://arxiv.org/pdf/1706.05394},
  abstract  = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns ﬁrst. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasetswithoutcompromisinggeneralizationon real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performanceofdeepnetworkswhentrainedwith gradient based methods because training data itself plays an important role in determining the degree of memorization.
},
  comment   = {DNN先学习后记忆},
  file      = {:paper/ACloserLookatMemorizationinDeepNetworks.pdf:PDF},
  keywords  = {rank3},
  owner     = {NULL},
  timestamp = {2018-03-26},
}

@InProceedings{Ioffe2015,
  author    = {Ioffe, Sergey and Szegedy, Christian},
  title     = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  booktitle = {NULL},
  year      = {2015},
  language  = {English},
  volume    = {1},
  note      = {Classification models;Covariate shifts;Deep neural networks;Higher learning;Learning rates;Model architecture;Network training;State of the art;},
  pages     = {448-456},
  url       = {https://www.engineeringvillage.com/search/doc/abstract.url?&pageType=quickSearch&usageZone=resultslist&usageOrigin=searchresults&searchtype=Quick&SEARCHID=fe1865a8Ma2d0M4c74Ma439M0d1910fd4650&DOCINDEX=1&ignore_docid=cpx_626caa911554ab001deM7b6c10178163171&database=1&format=quickSearchAbstractFormat&tagscope=&displayPagination=yes},
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  address   = {Lile, France},
  comment   = {Batch Normalization},
  copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
  file      = {:paper/BatchNormalization.pdf:PDF},
  journal   = {32nd International Conference on Machine Learning, ICML 2015},
  key       = {Image classification},
  keywords  = {Artificial intelligence;Learning systems;, rank5},
  owner     = {[A]ICML},
  timestamp = {2018-05-13},
}

@Article{Efficien95:online,
  author       = {Rui Min},
  title        = {Efficient Detection of Occlusion prior to Robust Face Recognition},
  journal      = {null},
  year         = {2014},
  note         = {(Accessed on 07/07/2018)},
  url          = {https://www.hindawi.com/journals/tswj/2014/519158/abs/},
  comment      = {人脸检测{遮挡和人脸在上下不同部分，再提取未遮挡人脸进行特征选取分类}},
  file         = {:paper/EfficientDetectionofOcclusionPriorToRobustFaceRecongnition.pdf:PDF},
  howpublished = {\url{https://www.hindawi.com/journals/tswj/2014/519158/abs/}},
  keywords     = {rank2},
  timestamp    = {2018-07-07},
}

@InProceedings{20181805133229,
  author    = {Singh, Amarjot and Patii, Devendra and Reddy, G. Meghana and Omkar, S.N.},
  title     = {Disguised Face Identification (DFI) with Facial KeyPoints Using Spatial Fusion Convolutional Network},
  booktitle = {null},
  year      = {2018},
  language  = {English},
  volume    = {2018-January},
  note      = {Annotated datasets;Classification methods;Classification performance;Convolutional networks;Face identification;Keypoint detection;Learning architectures;Learning frameworks;},
  pages     = {1648 - 1655},
  url       = {http://dx.doi.org/10.1109/ICCVW.2017.193},
  abstract  = {Disguised face identification (DFI) is an extremely challenging problem due to the numerous variations that can be introduced using different disguises. This paper introduces a deep learning framework to first detect 14 facial key-points which are then utilized to perform disguised face identification. Since the training of deep learning architectures relies on large annotated datasets, two annotated facial key-points datasets are introduced. The effectiveness of the facial keypoint detection framework is presented for each keypoint. The superiority of the key-point detection framework is also demonstrated by a comparison with other deep networks. The effectiveness of classification performance is also demonstrated by comparison with the state-of-the-art face disguise classification methods.<br/> &copy; 2017 IEEE.},
  address   = {Venice, Italy},
  comment   = {使用CNN识别人脸14个关键点},
  copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
  file      = {:paper/Singh_Disguised_Face_Identification_ICCV_2017_paper.pdf:PDF},
  journal   = {Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
  key       = {Face recognition},
  keywords  = {Computer vision;Damage detection;Deep learning;, rank3},
  owner     = {[C]ICCVW 2017},
}

@Article{ZHANG20171,
  author    = {Hengmin Zhang and Jian Yang and Jianchun Xie and Jianjun Qian and Bob Zhang},
  title     = {Weighted sparse coding regularized nonconvex matrix regression for robust face recognition},
  journal   = {Information Sciences},
  year      = {2017},
  volume    = {394-395},
  pages     = {1 - 17},
  issn      = {0020-0255},
  doi       = {https://doi.org/10.1016/j.ins.2017.02.020},
  url       = {http://www.sciencedirect.com/science/article/pii/S0020025517304954},
  comment   = {[牛] -WSC能将位置结构和相似关系结合成稀疏表示},
  keywords  = {Nonconvex matrix regression, Weighted sparse coding, Inexact augmented lagrange multiplier method, Face recognition, rank5},
  owner     = {[A]Information Sciences},
  timestamp = {2018-07-07},
}

@Article{726791,
  author    = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
  title     = {Gradient-based learning applied to document recognition},
  journal   = {Proceedings of the IEEE},
  year      = {1998},
  volume    = {86},
  number    = {11},
  month     = {Nov},
  pages     = {2278-2324},
  issn      = {0018-9219},
  doi       = {10.1109/5.726791},
  comment   = {经典论文LeNet},
  file      = {:paper/LeNet.pdf:PDF},
  keywords  = {backpropagation;convolution;multilayer perceptrons;optical character recognition;2D shape variability;GTN;back-propagation;cheque reading;complex decision surface synthesis;convolutional neural network character recognizers;document recognition;document recognition systems;field extraction;gradient based learning technique;gradient-based learning;graph transformer networks;handwritten character recognition;handwritten digit recognition task;high-dimensional patterns;language modeling;multilayer neural networks;multimodule systems;performance measure minimization;segmentation recognition;Character recognition;Feature extraction;Hidden Markov models;Machine learning;Multi-layer neural network;Neural networks;Optical character recognition software;Optical computing;Pattern recognition;Principal component analysis, rank5},
  owner     = {[A]IEEE},
  timestamp = {2018-07-11},
}

@Article{DBLP:journals/corr/SimonyanZ14a,
  author        = {Karen Simonyan and Andrew Zisserman},
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1409.1556},
  eprint        = {1409.1556},
  url           = {http://arxiv.org/abs/1409.1556},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
  comment       = {经典论文VGG},
  file          = {:paper/VGG.pdf:PDF},
  keywords      = {rank4},
  timestamp     = {Wed, 07 Jun 2017 01:00:00 +0200},
}

@Article{DBLP:journals/corr/ZhangYS17aa,
  author        = {Shuai Zhang and Lina Yao and Aixin Sun},
  title         = {Deep Learning based Recommender System: {A} Survey and New Perspectives},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1707.07435},
  eprint        = {1707.07435},
  url           = {http://arxiv.org/abs/1707.07435},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/ZhangYS17aa},
  comment       = {基于DL的推荐系统综述},
  file          = {基于深度学习的推荐系统综述:paper/DeepLearningbasedRecommenderSystemASurveyandNewPerspective.pdf:PDF},
  owner         = {[A]CoRR},
  timestamp     = {Tue, 08 Aug 2017 13:30:53 +0200},
}

@InProceedings{20160902042580,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title     = {Going deeper with convolutions},
  booktitle = {cvpr},
  year      = {2015},
  language  = {English},
  volume    = {07-12-June-2015},
  note      = {Architectural decision;Computational budget;Computing resource;Deep convolutional neural networks;Deep networks;Multi-scale;State of the art;Visual recognition;},
  pages     = {1-9},
  url       = {http://dx.doi.org/10.1109/CVPR.2015.7298594},
  abstract  = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.<br/> &copy; 2015 IEEE.},
  address   = {Boston, MA, United states},
  comment   = {经典论文Inception},
  copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
  file      = {:paper/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf:PDF},
  issn      = {10636919},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  key       = {Network architecture},
  keywords  = {Budget control;Computer vision;Convolution;Deep neural networks;Network layers;Neural networks;},
  owner     = {[C]CVPR},
  timestamp = {2018-08-03},
}

@InProceedings{Szegedy2015,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title     = {Going deeper with convolutions},
  booktitle = {cvpr},
  year      = {2015},
  language  = {English},
  volume    = {07-12-June-2015},
  note      = {Architectural decision;Computational budget;Computing resource;Deep convolutional neural networks;Deep networks;Multi-scale;State of the art;Visual recognition;},
  pages     = {1 - 9},
  url       = {http://dx.doi.org/10.1109/CVPR.2015.7298594},
  abstract  = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.<br/> &copy; 2015 IEEE.},
  address   = {Boston, MA, United states},
  comment   = {经典论文RestNet},
  copyright = {Compilation and indexing terms, Copyright 2018 Elsevier Inc.},
  file      = {:paper/ResNet_He_CVPR_2016.pdf:PDF},
  issn      = {10636919},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  key       = {Network architecture},
  keywords  = {Budget control;Computer vision;Convolution;Deep neural networks;Network layers;Neural networks;, rank4},
  owner     = {[C]CVPR},
  timestamp = {2018-08-03},
}

@InProceedings{Ding:2017:BFR:3132847.3132941,
  author    = {Ding, Daizong and Zhang, Mi and Li, Shao-Yuan and Tang, Jie and Chen, Xiaotie and Zhou, Zhi-Hua},
  title     = {BayDNN: Friend Recommendation with Bayesian Personalized Ranking Deep Neural Network},
  booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  year      = {2017},
  series    = {CIKM '17},
  publisher = {ACM},
  location  = {Singapore, Singapore},
  isbn      = {978-1-4503-4918-5},
  pages     = {1479--1488},
  doi       = {10.1145/3132847.3132941},
  url       = {http://doi.acm.org/10.1145/3132847.3132941},
  acmid     = {3132941},
  address   = {New York, NY, USA},
  comment   = {BayDNN网络结构进行好友推荐},
  file      = {:paper/BayDNN.pdf:PDF},
  keywords  = {bayesian personalized ranking deep neural network, pre-training strategy, probabilistic model, rank3},
  numpages  = {10},
  owner     = {[C]ACM},
}

@InBook{Bengio2006,
  author    = {Bengio, Yoshua and Schwenk, Holger and Sen{\'e}cal, Jean-S{\'e}bastien and Morin, Fr{\'e}deric and Gauvain, Jean-Luc},
  title     = {Neural Probabilistic Language Models},
  booktitle = {Innovations in Machine Learning: Theory and Applications},
  year      = {2006},
  editor    = {Holmes, Dawn E. and Jain, Lakhmi C.},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {978-3-540-33486-6},
  pages     = {137--186},
  doi       = {10.1007/3-540-33486-6_6},
  url       = {https://doi.org/10.1007/3-540-33486-6_6},
  abstract  = {A central goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on several methods to speed-up both training and probability computation, as well as comparative experiments to evaluate the improvements brought by these techniques. We finally describe the incorporation of this new language model into a state-of-the-art speech recognizer of conversational speech.},
  address   = {Berlin, Heidelberg},
  comment   = {经典NNLM模型},
  file      = {NLP-NNLM:paper/NLP/NNLM-ANeuralProbabilisticLanguageModel.pdf:PDF},
  keywords  = {rank4},
}

@Article{DBLP:journals/corr/abs-1301-3781,
  author        = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  title         = {Efficient Estimation of Word Representations in Vector Space},
  journal       = {CoRR},
  year          = {2013},
  volume        = {abs/1301.3781},
  eprint        = {1301.3781},
  url           = {http://arxiv.org/abs/1301.3781},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1301-3781},
  comment       = {经典word2vec},
  file          = {NLP-word2vec:paper/NLP/word2vec.pdf:PDF},
  keywords      = {rank4},
  timestamp     = {Mon, 13 Aug 2018 16:48:33 +0200},
}

@Article{DBLP:journals/tacl/LevyGD15,
  author    = {Omer Levy and Yoav Goldberg and Ido Dagan},
  title     = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
  journal   = {{TACL}},
  year      = {2015},
  volume    = {3},
  pages     = {211--225},
  url       = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/tacl/LevyGD15},
  comment   = {调参比扩展语料库有用},
  file      = {:paper/NLP/Improving Distributional Similarity with Lessons Learned from Word Embeddings.pdf:PDF},
  keywords  = {rank3},
  timestamp = {Thu, 28 May 2015 10:59:41 +0200},
}

@Article{DBLP:journals/jmlr/CollobertWBKKK11,
  author    = {Ronan Collobert and Jason Weston and L{\'{e}}on Bottou and Michael Karlen and Koray Kavukcuoglu and Pavel P. Kuksa},
  title     = {Natural Language Processing (Almost) from Scratch},
  journal   = {Journal of Machine Learning Research},
  year      = {2011},
  volume    = {12},
  pages     = {2493--2537},
  url       = {http://dl.acm.org/citation.cfm?id=2078186},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/jmlr/CollobertWBKKK11},
  comment   = {经典论文-利用一两个网络完成NLP的POS,Chunk, NER,SRL任务},
  file      = {:paper/NLP/Natural Language Processing (almost) from Scratch.pdf:PDF},
  keywords  = {rank3},
  timestamp = {Thu, 03 May 2012 15:41:30 +0200},
}

@Article{rumelhart1986learning,
  author      = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title       = {Learning representations by back-propagating errors},
  journal     = {Nature},
  year        = {1986},
  volume      = {323},
  month       = oct,
  pages       = {533--},
  url         = {http://dx.doi.org/10.1038/323533a0},
  added-at    = {2018-06-03T13:17:55.000+0200},
  biburl      = {https://www.bibsonomy.org/bibtex/25d95851c0f627ab11747a2e481ecbad6/achakraborty},
  comment     = {经典论文反向传播算法},
  description = {Learning representations by back-propagating errors | Nature},
  interhash   = {c354bc293fa9aa7caffc66d40a014903},
  intrahash   = {5d95851c0f627ab11747a2e481ecbad6},
  keywords    = {deep-learning nature neural-networks paper, rank5},
  publisher   = {Nature Publishing Group},
  timestamp   = {2018-06-03T13:17:55.000+0200},
}

@InProceedings{pmlr-v80-draxler18a,
  author    = {Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  title     = {Essentially No Barriers in Neural Network Energy Landscape},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
  month     = {10--15 Jul},
  pages     = {1309--1318},
  url       = {http://proceedings.mlr.press/v80/draxler18a.html},
  abstract  = {Training neural networks involves finding minima of a high-dimensional non-convex loss function. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essentially flat in both the training and test landscapes. This implies that minima are perhaps best seen as points on a single connected manifold of low loss, rather than as the bottoms of distinct valleys.},
  address   = {Stockholmsmässan, Stockholm Sweden},
  comment   = {[未细读]证明多维损失函数空间最优不是点，多个最优区域间是有路径的},
  file      = {draxler18a.pdf:http\://proceedings.mlr.press/v80/draxler18a/draxler18a.pdf:PDF},
}

@Book{DBLP:series/synthesis/2009Kubler,
  author    = {Sandra K{\"{u}}bler and Ryan T. McDonald and Joakim Nivre},
  title     = {Dependency Parsing},
  year      = {2009},
  series    = {Synthesis Lectures on Human Language Technologies},
  publisher = {Morgan {\&} Claypool Publishers},
  doi       = {10.2200/S00169ED1V01Y200901HLT002},
  url       = {https://doi.org/10.2200/S00169ED1V01Y200901HLT002},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/series/synthesis/2009Kubler},
  comment   = {[NLP][book]},
  timestamp = {Wed, 14 Nov 2018 10:12:17 +0100},
}

@Article{rong2014word2vec,
  author    = {Rong, Xin},
  title     = {word2vec parameter learning explained},
  journal   = {arXiv preprint arXiv:1411.2738},
  year      = {2014},
  added-at  = {2016-05-31T19:54:45.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/2d477c8671512a82ca12c30f8bec94ffd/albinzehe},
  comment   = {word2vec的详细公式推导和解释},
  file      = {:paper/word2vecParameterLearningExplained.pdf:PDF},
  interhash = {2254925bfa0cfc42fa403b528b5e47b0},
  intrahash = {d477c8671512a82ca12c30f8bec94ffd},
  keywords  = {thema:word_embeddings word2vec wordembeddings, rank4},
  timestamp = {2016-06-29T10:14:28.000+0200},
}

@Article{DBLP:journals/corr/abs-1801-00209,
  author        = {Xiangyu Zhao and Liang Zhang and Zhuoye Ding and Dawei Yin and Yihong Zhao and Jiliang Tang},
  title         = {Deep Reinforcement Learning for List-wise Recommendations},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1801.00209},
  eprint        = {1801.00209},
  url           = {http://arxiv.org/abs/1801.00209},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1801-00209},
  comment       = {强化学习做item推荐},
  file          = {:paper/RL/DeepReinforcementLearningforList-wiseRecommendations.pdf:PDF},
  keywords      = {rank3},
  timestamp     = {Mon, 13 Aug 2018 16:48:14 +0200},
}

@Article{DBLP:journals/corr/BahdanauCB14,
  author        = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  title         = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1409.0473},
  eprint        = {1409.0473},
  url           = {http://arxiv.org/abs/1409.0473},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/BahdanauCB14},
  comment       = {简单的Attention模型},
  file          = {:paper/NLP/NeuralMachineTranslationByJointlyLearningToAlignAndTranslate.pdf:PDF},
  timestamp     = {Mon, 13 Aug 2018 16:46:05 +0200},
}

@Article{Britz:2017,
  author        = {{Britz}, Denny and {Goldie}, Anna and {Luong}, Thang and {Le}, Quoc},
  title         = {{Massive Exploration of Neural Machine Translation Architectures}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = mar,
  eprint        = {1703.03906},
  eprinttype    = {arxiv},
  archiveprefix = {arXiv},
  comment       = {Google非官方产品,Attention。{Decoder的RNN门机制是必须的，双向比单向好，给RNN上残差并不好用，BeamSearch需要好的调参}},
  file          = {:paper/NLP/MassiveExplorationofNeuralMachineTranslation.pdf:PDF},
  keywords      = {Computer Science - Computation and Language},
  primaryclass  = {cs.CL},
}

@Comment{jabref-meta: databaseType:biblatex;}
