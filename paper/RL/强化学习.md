[TOC]

# 第一课 简介

 强化学习的难点：

- 没有监督，只有奖励信号
- 动作的反馈是滞后的
- 时间很重要(不同于机器学习中的数据同分布)



最大化累积收益期望 (奖励假说)，选择一个动作，可以是未来的整体酱紫最大化。

History和State

$H_t = A_1, O_1, R_1, \cdots , A_t, O_t, R_t$

$S_t=f(H_t)$

环境状态(environment state)$S_t^e$是环境的私有表现，通常是不可见的。即使可见，也可能包含不相关的信息。智能体状态(Agent state)$S_t^a$是agent的内部表示。  对目前的学习内容进行总结，然后采取下一步。



一个**信息状态**（马尔可夫状态, Markov state）包含历史中所有有用的信息。

定义状态$S_t$，当且仅当：

$\mathbb{P} [S_{t+1} | S_t] = \mathbb{P}[S_{t+1} | S_1,\cdots,S_t]$

下个状态仅与当前状态相关，与历史无关(马尔可夫性质)。

$H_{1:t}\rightarrow S_t \rightarrow H_{t+1:\infin}$

- 一旦状态已知，这样就把历史去掉。
- 状态是有足够的未来统计信息的。
- 环境状态$S_t^e$是马尔可夫。
- 历史$H_t$是马尔可夫。



**完全可观测(Full Observability)**：agent**直接**观测环境状态：

$O_t = S_t^a = S_t^e$

即 Agent状态=环境状态=信息状态。

更一般，这就是**马尔可夫决策(MDP)**



**部分观测(Partial observability)：** agent**间接**观察环境。

此时agent状态$\neq$环境状态。定义为，部分观测马尔可夫决策过程(**POMDP**)



RL的要素。

- 策略 Policy：agent的行为函数
- 值函数 Value function：每个动作或状态的好坏程度
- 模型 Model：agent的环境表示

决策策略：$a=\pi(s)$  , 随机策略 $\pi(a|s) = \mathbb{P}[A=a|S=s]$.

值函数是对未来奖励的预测，用来评估状态的好坏程度。定义：

$V_\pi (s)=\mathbb{E}_\pi [R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+\cdots | S_t = s]$

模型，预测环境的下一步骤。**转移(Transitions)**，$P$(动态)预测下一个状态，$R$预测下一个(即时)奖励值。这是部分并不是完全需要的。

$P_{ss^\prime}^a = \mathbb{P}[S^\prime=s^\prime | S = s, A = a]$

$R_s^a = \mathbb{E}[R|s=s,A=a]$



**RL中Agent的分类**：

- 基于值 (Value Based)

  - 使用值函数(value function), 策略不明确

- 基于策略 (Policy Based)

  - 策略 

  Agent存储的是policy而不是value

- AC Actor和Critic



RL的问题：

-  强化学习
  - 环境内部未知
  - agent与环境交互
  - agent提升自己的策略
- 规划问题
  - 环境的模型是已知的 
  -  agent与环境进行内部计算，不需要外部交互。



