[TOC]

# 1 简介

 强化学习的难点：

- 没有监督，只有奖励信号
- 动作的反馈是滞后的
- 时间很重要(不同于机器学习中的数据同分布)



最大化累积收益期望 (奖励假说)，选择一个动作，可以是未来的整体酱紫最大化。

History和State

$H_t = A_1, O_1, R_1, \cdots , A_t, O_t, R_t$

$S_t=f(H_t)$

环境状态(environment state)$S_t^e$是环境的私有表现，通常是不可见的。即使可见，也可能包含不相关的信息。智能体状态(Agent state)$S_t^a$是agent的内部表示。  对目前的学习内容进行总结，然后采取下一步。



一个**信息状态**（马尔可夫状态, Markov state）包含历史中所有有用的信息。

定义状态$S_t$，当且仅当：

$\mathbb{P} [S_{t+1} | S_t] = \mathbb{P}[S_{t+1} | S_1,\cdots,S_t]$

下个状态仅与当前状态相关，与历史无关(马尔可夫性质)。

$H_{1:t}\rightarrow S_t \rightarrow H_{t+1:\infin}$

- 一旦状态已知，这样就把历史去掉。
- 状态是有足够的未来统计信息的。
- 环境状态$S_t^e$是马尔可夫。
- 历史$H_t$是马尔可夫。



**完全可观测(Full Observability)**：agent**直接**观测环境状态：

$O_t = S_t^a = S_t^e$

即 Agent状态=环境状态=信息状态。

更一般，这就是**马尔可夫决策(MDP)**



**部分观测(Partial observability)：** agent**间接**观察环境。

此时agent状态$\neq$环境状态。定义为，部分观测马尔可夫决策过程(**POMDP**)



RL的要素。

- 策略 Policy：agent的行为函数
- 值函数 Value function：每个动作或状态的好坏程度
- 模型 Model：agent的环境表示

决策策略：$a=\pi(s)$  , 随机策略 $\pi(a|s) = \mathbb{P}[A=a|S=s]$.

值函数是对未来奖励的预测，用来评估状态的好坏程度。定义：

$V_\pi (s)=\mathbb{E}_\pi [R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+\cdots | S_t = s]$

模型，预测环境的下一步骤。**转移(Transitions)**，$P$(动态)预测下一个状态，$R$预测下一个(即时)奖励值。这是部分并不是完全需要的。

$P_{ss^\prime}^a = \mathbb{P}[S^\prime=s^\prime | S = s, A = a]$

$R_s^a = \mathbb{E}[R|s=s,A=a]$



**RL中Agent的分类**：

- 基于值 (Value Based)

  - 使用值函数(value function), 策略不明确

- 基于策略 (Policy Based)

  - 策略 

  Agent存储的是policy而不是value

- AC Actor和Critic



RL的问题：

-  强化学习
  - 环境内部未知
  - agent与环境交互
  - agent提升自己的策略
- 规划问题
  - 环境的模型是已知的 
  -  agent与环境进行内部计算，不需要外部交互。



# 2 马尔可夫决策过程(MDP)

## 马尔可夫过程

 马尔可夫性质：未来状态仅与当前状态有关，与历史无关。

状态转移矩阵。

马尔可夫过程是一个随机过程。定义：

马尔可夫过程(马尔可夫链)是一个元组$(\mathcal{S}, \mathcal{P})$, $\mathcal{S}$是状态集，$\mathcal{P}$是转移概率矩阵。

## 马尔可夫奖励过程

马尔可夫奖励过程(马尔可夫链)是一个元组$(\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma)$, $\mathcal{S}$是状态集，$\mathcal{P}$是转移概率矩阵，$\mathcal{P}_{ss^\prime} = \mathbb{P}[S_{t+1} = s^\prime | S_t = s]$, $\mathcal{R}$是奖励函数$R_s = \mathbb{E}[R_{t+1}|S_t = s]$，$\gamma$是折扣因子$\gamma \in [0,1]$。

定义 return $G_t$是在时间t的整体折扣奖励：

$G_{t} = R_{t+1} + \gamma R_{t+1} + \cdots = \sum_{k=0}^\infin \gamma^k R_{t+k+1}$

相比于未来reward，更倾向现在的即时reward。$\gamma$为0，则表示只关心即时奖励，设置为1，是考虑最长奖励。  

定义 状态值函数$v(s)$，是从状态s开始的期望：

$v(s) = \mathbb{E}[G_t | S_t = s]$

**Bellman方程**，思想是递归对值函数(value 函数)$v(s)$进行递归分解：

值函数(value function)能分解成两部分

- 即时奖励$R_{t+1}$
- 一系列的折扣奖励$\gamma v(S_{t+1})$

 $\begin{aligned} v(s) &=\mathbb{E}\left[G_{t} | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma G_{t+1} | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma v\left(S_{t+1}\right) | S_{t}=s\right] \end{aligned}$

**Bellman方程**，

$v(s)=\mathbb{E}\left[R_{t+1}+\gamma v\left(S_{t+1}\right) | S_{t}=s\right]$

$v(s)=\mathcal{R}_{s}+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{s s^{\prime}} v\left(s^{\prime}\right)$

**Bellman方程**矩阵向量化表示：

$v=\mathcal{R}+\gamma \mathcal{P} v$

$\left[\begin{array}{c}{v(1)} \\ {\vdots} \\ {v(n)}\end{array}\right]=\left[\begin{array}{c}{\mathcal{R}_{1}} \\ {\vdots} \\ {\mathcal{R}_{n}}\end{array}\right]+\gamma\left[\begin{array}{ccc}{\mathcal{P}_{11}} & {\dots} & {\mathcal{P}_{1 n}} \\ {\vdots} & {} & {} \\ {\mathcal{P}_{11}} & {\dots} & {\mathcal{P}_{n n}}\end{array}\right]\left[\begin{array}{c}{v(1)} \\ {\vdots} \\ {v(n)}\end{array}\right]$

Bellman方程是一个线性方程，所以能直接求解。

$\begin{aligned} v &=\mathcal{R}+\gamma P v \\(I-\gamma \mathcal{P}) v &=\mathcal{R} \\ v &=(I-\gamma \mathcal{P})^{-1} \mathcal{R} \end{aligned}$

> 如果有n个状态，求逆的复杂度为n的立方。所以大规模下，MDP并不适用。

## 马尔可夫决策过程

> 策略体现在状态间的概率在不同策略下是不同的，调整概率分布，不同的概率分布则对应不同的马尔可夫奖励过程。



定义 马尔可夫决策过程是四元组$(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$，其中$\mathcal{A}$是有限的动作集合，转移概率矩阵和奖励都添加了a标识：

 $\mathcal{P}_{s s^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} | S_{t}=s, A_{t}=a\right]$

$\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} | S_{t}=s, A_{t}=a\right]$

定义策略 **policy** $\pi$，它是在给定状态s下在所有action上的分布：

$\pi(a | s)=\mathbb{P}\left[A_{t}=a | S_{t}=s\right]$

在MDP中，不管的哪个时间步(到这个状态)，我们采取的措施都是一样的。 

MDP能最大化未来奖励。



在给定MDP $\mathcal{M} = <\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma>$和策略$\pi$，



- 状态序列 $S_1, S_2,\cdots$ 是一个马尔可夫过程 $<\mathcal{S, P^\pi}>$

- 如果固定策略，只研究状态序列和奖励，会发现这就是马尔可夫奖励过程$(\mathcal{S}, \mathcal{P}^\pi, \mathcal{R}^\pi, \gamma)$

- 其中

  - $\mathcal{P}_{s, s^{\prime}}^{\pi}=\sum_{a \in \mathcal{A}} \pi(a | s) \mathcal{P}_{s, s^{\prime}}^{a}$
  - $\mathcal{R}_{s}^{\pi}=\sum_{a \in \mathcal{A}} \pi(a | s) R_{s}^{a}$

  

定义 MDP的**状态值函数(state-value function)**  $v_\pi(s)$，其目标是返回从状态s开始，在策略$\pi$下，

$$v_{\pi}(s)=\mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right]$$

 

定义 MDP中的 **动作值函数(action-value function)** 

$q_\pi (s,a)=\mathbb{E}_\pi[G_t|S_t=s, A_t=a]$



MDP中的**Bellman方程**

状态值函数能再一次分解成即时奖励加上一系列状态的折扣奖励

$v_{\pi}(s)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) | S_{t}=s\right]$

同样，动作值函数能以相似的方式展开

$q_{\pi}(s, a)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) | S_{t}=s, A_{t}=0\right]$



$v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a | s) q_{\pi}(s, a)$

$q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{s s^{s}}^{a} v_{\pi}\left(s^{\prime}\right)$

v告诉我们它处于一个特定state时，这个state有多好。q告诉我们采取一个给定a的好坏程度。将二者合并起来：

$v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a | s)\left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\right)$

主要思想是当前时刻的即时value function的reward加上结束时候的value function的reward。



Bellman期望方程可以用MRP来表示：

$v_{\pi}=\mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} v_{\pi}$

同样求解可得：

$v_{\pi}=\left(I-\gamma P^{\pi}\right)^{-1} \mathcal{R}^{\pi}$



> **Bellman是一种解决方法，通过线性方程求解从而得到value function**
>
> 从MRP转移到MDP时候，改变了问题的定义。前者是状态state，后者是决策。



**最优值函数**

 最优状态值函数$V_{*}$是在所有策略中，最大化值函数的：

$V_{*}(s)=\max _{\pi} v_{\pi}(s)$

最优动作值函数$q_{*}$是在所有的策略中，最大化值函数的：

$q_{*}(s, a)=\max _{\pi} q_{\pi}(s, a)$



**最优策略 **

 **两个策略的比较**

定义 在所有策略上的偏序

$\pi \geq \pi^{\prime}$ if $v_{\pi}(s) \geq v_{\pi^{\prime}}(s), \forall s$

> 对于任意的状态，策略1的值函数都大于策略2的值函数

这样最优策略可以通过最大化动作值函数$q_{*}(s, a)$来实现，

$\pi_{*}(a | s)=\left\{\begin{array}{ll}{1} & {\text { if } a=\underset{a \in A}{\partial \in A} q_{*}(s, a)} \\ {0} & {\text { otherwise }}\end{array}\right.$



最优值函数是递归相关的Bellman最优公式：

$V_{*}(s)=\max _{a} q_{*}(s, a)$

$q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)$

$v_{*}(s)=\max _{a} \mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)$



Bellman 寻优公式不是线性的，解决方式有**1. value iteration 2. Policy Iteration 3. Q-learning 4. Sarsa**




## MDP的扩展

无限和连续的MDPs

部分可观测MDPs

没有折扣，平均奖励的MDPs