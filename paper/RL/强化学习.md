[TOC]

# 1 简介

 强化学习的难点：

- 没有监督，只有奖励信号
- 动作的反馈是滞后的
- 时间很重要(不同于机器学习中的数据同分布)



最大化累积收益期望 (奖励假说)，选择一个动作，可以是未来的整体奖励最大化。

History和State

$H_t = A_1, O_1, R_1, \cdots , A_t, O_t, R_t$

$S_t=f(H_t)$

环境状态(environment state)$S_t^e$是环境的私有表现，通常是不可见的。即使可见，也可能包含不相关的信息。智能体状态(Agent state)$S_t^a$是agent的内部表示。  对目前的学习内容进行总结，然后采取下一步。



一个**信息状态**（马尔可夫状态, Markov state）包含历史中所有有用的信息。

定义状态$S_t$，当且仅当：

$\mathbb{P} [S_{t+1} | S_t] = \mathbb{P}[S_{t+1} | S_1,\cdots,S_t]$

下个状态仅与当前状态相关，与历史无关(马尔可夫性质)。

$H_{1:t}\rightarrow S_t \rightarrow H_{t+1:\infin}$

- 一旦状态已知，这样就把历史去掉。
- 状态是有足够的未来统计信息的。
- 环境状态$S_t^e$是马尔可夫。
- 历史$H_t$是马尔可夫。



**完全可观测(Full Observability)**：agent**直接**观测环境状态：

$O_t = S_t^a = S_t^e$

即 Agent状态=环境状态=信息状态。

更一般，这就是**马尔可夫决策(MDP)**



**部分观测(Partial observability)：** agent**间接**观察环境。

此时agent状态$\neq$环境状态。定义为，部分观测马尔可夫决策过程(**POMDP**)



RL的要素。

- 策略 Policy：agent的行为函数
- 值函数 Value function：每个动作或状态的好坏程度
- 模型 Model：agent的环境表示

决策策略：$a=\pi(s)$  , 随机策略 $\pi(a|s) = \mathbb{P}[A=a|S=s]$.

值函数是对未来奖励的预测，用来评估状态的好坏程度。定义：

$V_\pi (s)=\mathbb{E}_\pi [R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+\cdots | S_t = s]$

模型，预测环境的下一步骤。**转移(Transitions)**，$P$(动态)预测下一个状态，$R$预测下一个(即时)奖励值。这是部分并不是完全需要的。

$P_{ss^\prime}^a = \mathbb{P}[S^\prime=s^\prime | S = s, A = a]$

$R_s^a = \mathbb{E}[R|s=s,A=a]$



**RL中Agent的分类**：

- 基于值 (Value Based)

  - 使用值函数(value function), 策略不明确

- 基于策略 (Policy Based)

  - 策略 

  Agent存储的是policy而不是value

- AC Actor和Critic



RL的问题：

-  强化学习
  - 环境内部未知
  - agent与环境交互
  - agent提升自己的策略
- 规划问题
  - 环境的模型是已知的 
  -  agent与环境进行内部计算，不需要外部交互。



# 2 马尔可夫决策过程(MDP)

## 马尔可夫过程

 马尔可夫性质：未来状态仅与当前状态有关，与历史无关。

状态转移矩阵。

马尔可夫过程是一个随机过程。定义：

马尔可夫过程(马尔可夫链)是一个元组$(\mathcal{S}, \mathcal{P})$, $\mathcal{S}$是状态集，$\mathcal{P}$是转移概率矩阵。

## 马尔可夫奖励过程

马尔可夫奖励过程(马尔可夫链)是一个元组$(\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma)$, $\mathcal{S}$是状态集，$\mathcal{P}$是转移概率矩阵，$\mathcal{P}_{ss^\prime} = \mathbb{P}[S_{t+1} = s^\prime | S_t = s]$, $\mathcal{R}$是奖励函数$R_s = \mathbb{E}[R_{t+1}|S_t = s]$，$\gamma$是折扣因子$\gamma \in [0,1]$。

定义 return $G_t$是在时间t的整体折扣奖励：

$G_{t} = R_{t+1} + \gamma R_{t+1} + \cdots = \sum_{k=0}^\infin \gamma^k R_{t+k+1}$

相比于未来reward，更倾向现在的即时reward。$\gamma$为0，则表示只关心即时奖励，设置为1，是考虑最长奖励。  

定义 状态值函数$v(s)$，是从状态s开始的期望：

$v(s) = \mathbb{E}[G_t | S_t = s]$

**Bellman方程**，思想是递归对值函数(value 函数)$v(s)$进行递归分解：

值函数(value function)能分解成两部分

- 即时奖励$R_{t+1}$
- 一系列的折扣奖励$\gamma v(S_{t+1})$

 $\begin{aligned} v(s) &=\mathbb{E}\left[G_{t} | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma G_{t+1} | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma v\left(S_{t+1}\right) | S_{t}=s\right] \end{aligned}$

**Bellman方程**，

$v(s)=\mathbb{E}\left[R_{t+1}+\gamma v\left(S_{t+1}\right) | S_{t}=s\right]$

$v(s)=\mathcal{R}_{s}+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{s s^{\prime}} v\left(s^{\prime}\right)$

**Bellman方程**矩阵向量化表示：

$v=\mathcal{R}+\gamma \mathcal{P} v$

$\left[\begin{array}{c}{v(1)} \\ {\vdots} \\ {v(n)}\end{array}\right]=\left[\begin{array}{c}{\mathcal{R}_{1}} \\ {\vdots} \\ {\mathcal{R}_{n}}\end{array}\right]+\gamma\left[\begin{array}{ccc}{\mathcal{P}_{11}} & {\dots} & {\mathcal{P}_{1 n}} \\ {\vdots} & {} & {} \\ {\mathcal{P}_{11}} & {\dots} & {\mathcal{P}_{n n}}\end{array}\right]\left[\begin{array}{c}{v(1)} \\ {\vdots} \\ {v(n)}\end{array}\right]$

Bellman方程是一个线性方程，所以能直接求解。

$\begin{aligned} v &=\mathcal{R}+\gamma P v \\(I-\gamma \mathcal{P}) v &=\mathcal{R} \\ v &=(I-\gamma \mathcal{P})^{-1} \mathcal{R} \end{aligned}$

> 如果有n个状态，求逆的复杂度为n的立方。所以大规模下，MDP并不适用。

## 马尔可夫决策过程

> 策略体现在状态间的概率在不同策略下是不同的，调整概率分布，不同的概率分布则对应不同的马尔可夫奖励过程。



定义 马尔可夫决策过程是四元组$(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$，其中$\mathcal{A}$是有限的动作集合，转移概率矩阵和奖励都添加了a标识：

 $\mathcal{P}_{s s^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} | S_{t}=s, A_{t}=a\right]$

$\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} | S_{t}=s, A_{t}=a\right]$

定义策略 **policy** $\pi$，它是在给定状态s下在所有action上的分布：

$\pi(a | s)=\mathbb{P}\left[A_{t}=a | S_{t}=s\right]$

在MDP中，不管的哪个时间步(到这个状态)，我们采取的措施都是一样的。 

MDP能最大化未来奖励。



在给定MDP $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$和策略$\pi$，



- 状态序列 $S_1, S_2,\cdots$ 是一个马尔可夫过程 $\langle \mathcal{S, P^\pi} \rangle$

- 如果固定策略，只研究状态序列和奖励，会发现这就是马尔可夫奖励过程$(\mathcal{S}, \mathcal{P}^\pi, \mathcal{R}^\pi, \gamma)$

- 其中

  - $\mathcal{P}_{s, s^{\prime}}^{\pi}=\sum_{a \in \mathcal{A}} \pi(a | s) \mathcal{P}_{s, s^{\prime}}^{a}$
  - $\mathcal{R}_{s}^{\pi}=\sum_{a \in \mathcal{A}} \pi(a | s) R_{s}^{a}$

  

定义 MDP的**状态值函数(state-value function)**  $v_\pi(s)$，其目标是返回从状态s开始，在策略$\pi$下，

$$v_{\pi}(s)=\mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right]$$

 

定义 MDP中的 **动作值函数(action-value function)** 

$q_\pi (s,a)=\mathbb{E}_\pi[G_t|S_t=s, A_t=a]$



MDP中的**Bellman方程**

状态值函数能再一次分解成即时奖励加上一系列状态的折扣奖励

$v_{\pi}(s)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) | S_{t}=s\right]$

同样，动作值函数能以相似的方式展开

$q_{\pi}(s, a)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) | S_{t}=s, A_{t}=0\right]$



$$v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a | s) q_{\pi}(s, a) \tag{1}$$

$$q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{s s^{s}}^{a} v_{\pi}\left(s^{\prime}\right) \tag{2}$$

v告诉我们它处于一个特定state时，这个state有多好。q告诉我们采取一个给定a的好坏程度。将二者合并起来：

$v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a | s)\left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\right) \tag{3}$

> 将(2)代入到(1)，可以得到公式(3)。  在给定动作a下，这是马尔可夫奖励过程，所以$q_\pi$公式与MRP的值函数公式一样。而决策体现在不同的动作的概率分布。在一个策略下，策略$\pi$在所有动作上的概率分布是已知的，并且确定动作后的值函数也已经求得(MRP)。所以接下来来定义MDP中的状态值函数，$P\cdot V$。用每个动作的概率乘以每个动作下的值函数，然后求和，得到当前策略的值函数。
>
> 
>
> 1. 状态值函数V：从状态x0 开始, 所有的动作a，都是执行某个策略π的结果，最后求每个动作带来累积奖赏；
>
> 2. 状态-动作函数Q：从状态x0开始，先执行动作a0， 然后再执行某个策略π，再求相应的积累奖赏。
>
> 
>
> **某一个状态的价值可以用该状态下所有动作的价值表述。**
>
> **某一个动作的价值可以用该状态后续状态的价值表达。**

主要思想是当前时刻的即时value function的reward加上结束时候的value function的reward。



Bellman期望方程可以用MRP来表示：

$v_{\pi}=\mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} v_{\pi}$

同样求解可得：

$v_{\pi}=\left(I-\gamma P^{\pi}\right)^{-1} \mathcal{R}^{\pi}$



> **Bellman是一种解决方法，通过线性方程求解从而得到value function**
>
> 从MRP转移到MDP时候，改变了问题的定义。前者是状态state，后者是决策。



**最优值函数**

 最优状态值函数$V_{*}$是在所有策略中，最大化值函数的：

$V_{*}(s)=\max _{\pi} v_{\pi}(s)$

最优动作值函数$q_{*}$是在所有的策略中，最大化值函数的：

$q_{*}(s, a)=\max _{\pi} q_{\pi}(s, a)$



**最优策略 **

 **两个策略的比较**

定义 在所有策略上的偏序

$\pi \geq \pi^{\prime}$ if $v_{\pi}(s) \geq v_{\pi^{\prime}}(s), \forall s$

> 对于任意的状态，策略1的值函数都大于策略2的值函数

这样最优策略可以通过最大化动作值函数$q_{*}(s, a)$来实现，

$\pi_{*}(a | s)=\left\{\begin{array}{ll}{1} & {\text { if } a=\underset{a \in A}{\partial \in A} q_{*}(s, a)} \\ {0} & {\text { otherwise }}\end{array}\right.$



最优值函数是递归相关的Bellman最优公式：

$V_{*}(s)=\max _{a} q_{*}(s, a)$

$q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)$

$v_{*}(s)=\max _{a} \mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)$



Bellman 寻优公式不是线性的，解决方式有**1. value iteration 2. Policy Iteration 3. Q-learning 4. Sarsa**




## MDP的扩展

无限和连续的MDPs

部分可观测MDPs

没有折扣，平均奖励的MDPs



# 3 动态规划

动态规划的一般特性：

最优结构

重叠子问题



MDP的**预测(Prediction)**问题，可以理解为给定策略下的评估:

- 输入 MDP$\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$和策略 policy $\pi$
- 输出值函数 $V_\pi$

MDP的**控制(Control)**问题，最优策略的寻找：

- 输入 MDP$\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$
- 输出最优值函数$V_*$



www.cs.ubc.ca/~poole/demos/mdp/vi.html

![](<https://img-blog.csdn.net/20170831203538565?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFuZ2xpbnpodW8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast>)

策略迭代包含两部分：策略评估 和 策略提示

而值迭代一步到位，只有策略评估，然后直接更新值。





| 问题 | Bellman方程                  | 算法         |
| ---- | ---------------------------- | ------------ |
| 预测 | Bellman期望方程              | 迭代策略评估 |
| 控制 | Bellman期望方程+贪婪策略提升 | 策略迭代     |
| 控制 | Bellman最优方程              | 值迭代       |



# 4 不基于模型的预测

## 蒙特卡洛强化学习

MC直接从经验中的episode学习，MC是无模型(不需要指导转移概率剧本和奖励)。 MC只能从完整的episode进行学习。

目标：从episode中以策略$\pi$学习$v_\pi$：

$S_{1}, A_{1}, R_{2}, \dots, S_{k} \sim \pi$

计算所有折扣奖励

$G_{t}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{T-1} R_{T}$

通过期望计算值函数

$v_{\pi}(s)=\mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right]$



初次访问-蒙特卡洛/每次访问-蒙特卡洛

递增的平均

$\begin{aligned} \mu_{k} &=\frac{1}{k} \sum_{j=1}^{k} x_{j} \\ &=\frac{1}{k}\left(x_{k}+\sum_{j=1}^{k-1} x_{j}\right) \\ &=\frac{1}{k}\left(x_{k}+(k-1) \mu_{k-1}\right) \\ &=\mu_{k-1}+\frac{1}{k}\left(x_{k}-\mu_{k-1}\right) \end{aligned}$



**递增的蒙特卡洛更新**

- 在episode $S_{1}, A_{1}, R_{2}, \ldots, S_{T}$，递增更新$V(s)$

- 对每个状态$S_t$及反馈$G_t$

  $N\left(S_{t}\right) \leftarrow N\left(S_{t}\right)+1$

  $V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\frac{1}{N\left(S_{t}\right)}\left(G_{t}-V\left(S_{t}\right)\right)$

- 如果计数不好统计可以使用

  $V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(G_{t}-V\left(S_{t}\right)\right)$



## 时序差分学习(TD)

蒙特卡洛的更新是使用真实的反馈进行更新：

$V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(G_{t}-V\left(S_{t}\right)\right)$

> $\left(G_{t}-V\left(S_{t}\right)\right)$ 

而TD(0)中，是估计反馈：$R_{t+1}+\gamma V\left(S_{t+1}\right)$ (称为TD目标值)，再用它替代上式中的$G_t$

$V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\right)$

> $R_{t+1} + \gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)$ 称为TD错误

